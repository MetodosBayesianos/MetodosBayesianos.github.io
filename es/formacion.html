<!DOCTYPE html>
  <html>
    <head>
      <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.8.2/css/all.css" integrity="sha384-oS3vJWv+0UjzBfQzYUhtDYW+Pj2yciDJxpsK1OYPAYjqT085Qq/1cq5FLXAZQ7Ay" crossorigin="anonymous">
      <link href="https://fonts.googleapis.com/icon?family=Material+Icons" rel="stylesheet">
      <link href="../css/materialize.css" type="text/css" rel="stylesheet" media="screen,projection"/>
      <link type="text/css" rel="stylesheet" href="../css/materialize.min.css"  media="screen,projection"/>
      <title>Métodos</title>
      <link rel="icon" href="logos/favicon.png" type="image/x-icon">
      <!--Let browser know website is optimized for mobile-->
      <link rel="stylesheet" href="../css/others.css">
      <meta name="viewport" content="width=device-width, initial-scale=1.0"/>
    </head>

    <body>
        <!-- BEGIN HEADER -->
        <ul id="dropdown0" class="dropdown-content">
            <li><a href=""></a></li>
            <li><a href="../es/index.html">Español</a></li>
            <li><a href="../en/index.html">English</a></li>
        </ul>
        <ul id="dropdown1" class="dropdown-content">
            <li><a href=""></a></li>
            <li><a href="../es/index.html">Español</a></li>
            <li><a href="../en/index.html">English</a></li>
        </ul>

        <nav class="deep-orange" role="navigation" style="position:fixed">
            <div class="nav-wrapper container"><a id="logo-container" href="../es/index.html" class="brand-logo"><em>Métodos</em></a>
            <ul class="right hide-on-med-and-down">
                <li><a href="../es/fundamental.html"><b>Elementos</b></a></li>
                <li><a href="../es/comunidad.html"><b>Comunidad</b></a></li>
                <li><a href="../es/aplicaciones.html"><b>Aplicaciones</b></a></li>
                <li><a href="../es/formacion.html"><b>Formación</b></a></li>
                <li><a href="../es/personal.html"><b>Personal</b></a></li>
                <li><a href="../es/competencia.html"><b>Competencia</b></a></li>
                <li><a href="../es/sede.html"><b>Contacto</b></a></li>
                <!--<li><a class="dropdown-button" href="#!" data-target="dropdown0"><i class="fa fa-language"></i></a></li>-->
            </ul>
        </nav>

        <nav class="deep-orange" role="navigation">
            <ul id="nav-mobile" class="sidenav">
                <!--<li><a class="dropdown-button" href="#!" data-target="dropdown1"><i class="fa fa-language"></i></a></li>-->
                <li><a href="../es/index.html">Métodos Bayesianos</a></li>
                <li><a href=""></a></li>
                <li><a href="../es/fundamental.html">&#x2022; Elementos</a></li>
                <li><a href="../es/comunidad.html">&#x2022; Comunidad</a></li>
                <li><a href="../es/aplicaciones.html">&#x2022; Aplicaciones</a></li>
                <li><a href="../es/formacion.html">&#x2022; Formación</a></li>
                <li><a href="../es/personal.html">&#x2022; Personal</a></li>
                <li><a href="../es/competencia.html">&#x2022; Competencia</a></li>
                <li><a href="../es/sede.html">&#x2022; Contacto</a></li>
            </ul>
            <a href="#" data-target="nav-mobile" class="sidenav-trigger"><i class="material-icons">menu</i></a>
            </div>
        </nav>
        <!-- END HEADER -->


        <div class="section no-pad-bot" id="index-banner">
            <div class="container" >
            <br><br>

                <h4 class="header col s12 light">Formación en Ciencia de Datos Causal Bayesiana</h4>

                <p>
                REQUISITOS DE INGRESO: Las personas que ingresan deben tener un conocimiento mínimo de programación y matemática. No se requiere conocimientos en probabilidad y estadística.
                La carga horaria total de la formación es de 144 horas y se dicta en 6 meses.
                </p>

                <h5 class="header col s12 light">
                Fines y objetivos que pretende alcanzar.
                </h5>

                <p>
                Varias décadas de investigación en el campo del aprendizaje automático han dado lugar a una multitud de algoritmos diferentes para resolver una amplia gama de problemas. Para abordar un nuevo proyecto bajo este enfoque, un investigador debe mapear su problema con alguno de los métodos ya existentes. Esto es a menudo complicado por la variedad de algoritmos, siempre diferentes, cada uno con sus especificidades, que requieren generalmente implementaciones de software proveídos por terceros. Si bien este flujo de trabajo tradicional ha sido exitoso, es al mismo tiempo muy inflexible, especialmente para quienes no tienen una formación en matemática y programación.
                </p>

                <p>
                En esas mismas décadas se fueron desarrollando técnicas generales, que permiten aprovechar toda la potencia del aprendizaje automático de forma dinámica e intuitiva, permitiendo crear modelos personalizados a la medida de cada problema específico. A esto le llaman "Aprendizaje automático basado en modelos". Las técnicas las podemos resumir en tres. Una para especificar modelos intuitivamente (1) y otras dos técnicas para resolver la inferencia de forma sencilla, una para cuando podemos resolver la matemática (2) y otra cuando no podemos (3). Especificar modelos y resolver la inferencia es todo lo que hay que hacer en ciencia de datos.
                </p>

                <p>
                <!--Simplemente es aplicar las reglas de la probabilidad: la regla de la suma, hacer las predicciones con la contribución de todas las hipótesis; y la regla del producto, preservar la creencia previa que sigue siendo compatible con el dato. Estas reglas se las conoce hace 250 años y desde entonces no se encontró nada mejor.-->
                El problema histórico de la teoría de la probabilidad es que nos obliga a evaluar todas y cada una de las hipótesis, lo que es computacionalmente costoso. Desde su descubrimiento se fueron desarrollando soluciones parciales a este problema. A finales del siglo 19 la física estadística logra resolver este problema para un conjunto de problemas. En el siglo 20, el enfoque frecuentista busca desarrollar métodos robustos que seleccionan cuando se selecciona una única hipótesis (e.g. por máxima verosimilitud). No fue posible hacer la inferencia correctamente (evaluando todo el espacio de hipótesis) hasta que en las vísperas del siglo 21 se produce el enorme crecimiento de la capacidad de cómputo.
                </p>

                <h6 class="header col s12 light">
                Técnicas intuitivas y dinámicas
                </h6>

                <p>
                A finales del siglo pasado se desarrollaron las 3 técnicas generales son el el centro de la formación en ciencia de datos causal bayesiana.
                </p>

                <p>
                (1) La primera es la especificación de todas las hipótesis usando modelos causales gráficos. En ellos se dibuja una red, en el cual las variables quedan vinculadas por flechas que representan relaciones causales probabilísticas entre variables. Este método tiene varias ventajas. No sólo es intuitivo, es al mismo tiempo la especificación matemática de la distribución conjunta de todas nuestras hipótesis, el modelo. Pero además, está estrechamente relacionado con los dos métodos para resolver la inferencia.
                </p>
                <p>
                (2) La segunda es el algoritmo de pasaje de mensajes en modelos gráficos (factor graph). Este algoritmo, conocido como belief propagation o sum-product algorithm, simplemente descompone las dos reglas de la probabilidad en pasos elementales: los mensajes que envían las variables (product step), y los mensajes que envían las funciones (sum step). Es el algoritmo más eficiente posible para aplicar las reglas de la probabilidad cuando podemos resolver la matemática. Además se puede ejecutar de forma paralela.
                </p>
                <p>
                (3) La tercera son los lenguajes de programación probabilística. Cuando no podemos resolver la matemática, hoy contamos con algoritmos que nos pueden ayudar a aproximar la inferencia. La programación probabilística nos permite abstraernos de la mayor parte de los detalles detrás de esos algoritmos. Como usuarios, tenemos una sintaxis que nos permite traducir directamente la especificación gráfica del modelo que hicimos en (1). Y se requiere algunos conocimientos mínimos para asegurarse que el resultado que escupe tenga sentido, porque a veces los algoritmos tienen limitaciones.
                </p>

                <p>
                En la formación en Ciencia de Datos Causal Bayesiana aprendemos estas técnicas. Además de permitir resolver cualquier problema de ciencia de datos, nos enseña los conceptos fundamentales de la teoría de la probabilidad. Las personas que aprueben esta formación, van a estar capacitadas para formar parte de equipos de ciencia de datos en la industria, especialmente en proyectos sensibles como salud y ciencias forenses, donde realizar una correcta inferencia causal es fundamental.
                </p>

                <h5 class="header col s12 light">
                Programa.
                </h5>

                <p>
                EVALUACIÓN: A lo largo de toda la formación los estudiantes desarrollarán de forma paralela un aplicación práctica compleja. Al final del curso se tomará un único examen final.
                </p>

                <h6 class="header col s12 light">
                Módulo 1: Probabilidad, datos, modelos e inferencia
                </h6>

                <p>
                Las 36 horas cubren 6 semanas, con 6 horas por semana, 3 de teórica y 3 de práctica.
                </p>

                <p><b>Clases teóricas</b>: Definición de verdad en contextos de incertidumbre. Principios de máxima incertidumbre y coherencia. Las reglas de la probabilidad. El teorema de Bayes. Interpretación de la verosimilitud y la evidencia. Introducción a la notación de los métodos gráficos. Algunas distribuciones conjugadas. El problema de la comunicación con la realidad: teoría de la información. La estructura invariante del dato empírico y la equivalencia con el esquema emisor-receptor. Problemas de los enfoques ad-hoc que seleccionan una única hipótesis del espacio: sobreajuste y doble descenso. La justificación teórica de los conjuntos de datos de entrenamiento y testeo, y sus efectos positivos en la práctica.
                </p>

                <p><b>Clases práctica</b>: Un ejemplo por clase: Monty Hall, beta-binomial, modelo de habilidad-desempeño y regresión lineal bayesiana. Implementación por cuadratura del ejemplo beta-binomia y el modelo de habilidad - desempeño (gaussiana - gaussiana). Comparación con la solución exacta. Evaluación de regresiones lineales bayesianas de complejidad creciente sobre datos simulados. Selección de hipótesis por máxima verosimilitud y el problema del sobreajuste y donde descenso. Evaluación por validación cruzada y comparación con la evaluación de modelos por evidencia. Regresiones lineales sobre datos reales.
                </p>

                <h6 class="header col s12 light">
                Módulo 2: Modelos gráficos e inferencia causal.
                </h6>

                <p>
                Las 36 horas cubren 6 semanas, con 6 horas por semana, 3 de teórica y 3 de práctica.
                </p>

                <p><b>Clases teóricas</b>: El alto costo computacional de la aplicación estricta de la probabilidad. Cómputo eficiente en modelos gráficos probabilísticos a través de pasaje de mensaje distribuidos entre nodos de la red bayesiana (algoritmo suma-producto). Flujos de inferencia en modelos causales, dependencia e independencia condicional. Criterio d-separación. El concepto del operador “do”. Estimación de efectos causales con datos observables no experimentales. Controles backdoor y frontdoor.  Evaluación de modelos causales alternativos a través del factor de Bayes y la ventaja natural a favor de los modelos de complejidad intermedia.

                <p><b>Clases práctica</b>:
                1. Utilizar el algoritmo suma-producto para derivar a mano las dependencias e independencias causales en el modelo alarma-terremoto. Derivar las reglas de d-separación. Determinación de variables de control en estructuras causales genéricas. 2. Simulación de datos a través de modelos causales generativos, evaluación de modelos causales alternativos y estimación de efectos causales.  3. Pasaje de mensajes en el modelo de habilidad exacto, e implementación de la aproximación analítica para solución de filtrado.
                </p>

                <h6 class="header col s12 light">
                Módulo 3: Inferencia eficiente y programación probabilística
                </h6>

                <p>
                Las 36 horas cubren 6 semanas, con 6 horas por semana, 3 de teórica y 3 de práctica.
                </p>

                <p><b>Clases teóricas</b>: Modelos mixtos. Divergencia entre distribuciones de probabilidad. Métodos de aproximación analítica por minimización de divergencia: expectation propagation y variational inference. Las ventajas y limitaciones de las aproximaciones analíticas. Métodos de aproximación por muestreo. Las ventajas y desventajas. Markov Chain Monte Carlo Metropolis Algorithm en detalle. Ideas de Hamiltonian Monte Carlo y Sequential Monte Carlo. Ejemplo: diagnósticos de test tipo COVID con independencia y sin independencia.
                </p>

                <p><b>Clases práctica</b>:
                1. Implementación del algoritmo de sampleo metropolis. 2. Implementación de modelos mixtos gaussianos en PyMC y selección de modelo a través de sequential monte carlo.  3. Implementación en PyMC de estimación de prevalencia, sensibilidad y especificidad en base a resultados diagnósticos de test tipo PCR y test rápidos.
                </p>

                <h6 class="header col s12 light">
                Módulo 4: Series de tiempo y toma de decisiones
                </h6>

                <p>
                Las 36 horas cubren 6 semanas, con 6 horas por semana, 3 de teórica y 3 de práctica.
                </p>

                <p><b>Clases teóricas</b>:
                Hidden Markov Model (HMM). Conceptos de filtrado y smoothing en series de tiempo. El forward-backward algorithm. El algoritmo suma-producto para HMM. El algoritmo de convergencia por loopy belief propagation. Sistemas lineales dinámicos. Kernel methods. Gaussian processes. Toma de decisiones y maximización de la tasa de crecimiento temporal en procesos multiplicativos.
                Reformulación ergódica y apuestas óptimas bajo cualquier función de costo.
                </p>

                <p><b>Clases práctica</b>:
                1. Implementación propia del "Monty Hall extendido" estimar el sesgo de la persona que esconde el regalo a través del tiempo a través de un modelo dirichlet-multinomial. Comparación de las estimaciones obtenidas mediante filtrado y smoothing. 2. Competencia “inferencia con apuestas”, en el que una casa de apuestas  las personas tienen que apostar 3. Implementación del estimación de habilidad en la industria del video juego (TrueSkill Through Time) a partir de una programa semi terminado. 4. Estimación de habilidades deportivas y apuestas contra una base de datos de los pagos ofrecidos por una casa de apuestas.
                </p>

            </div>

        <br>
        <br>
        <br>
        <br>


        </div>


        <!--Footer-->
        <nav class="deep-orange" role="navigation" style="background-color:#1a1a1a; height:10%">
            <div class="nav-wrapper container">
                <ul class="social-links">
                    <div class="footer" >
                    <li><a href="mailto:metodosbayesianos@proton.me"><i class="fas fa-envelope"></i></a></li>
                    </div>
                </ul>
            </div>
        </nav>

        <!--JavaScript at end of body for optimized loading-->
        <script src="https://code.jquery.com/jquery-2.1.1.min.js"></script>
        <script src="../js/materialize.js"></script>
        <script src="../js/init.js"></script>
        <script>
            $('.dropdown-button').dropdown({
                inDuration: 300,
                outDuration: 225,
                constrain_width: false, // Does not change width of dropdown to that of the activator
                gutter: 0, // Spacing from edge
                alignment: 'left' // Displays dropdown with edge aligned to the left of button
                }
            );
        </script>
    </body>  

  </html>
 
